@preamble{{\providecommand*\hyphen{-}}}

@comment{
  Style Transferに関する研究
}
@inproceedings{10.1145/383259.383295,
  author    = {Hertzmann, Aaron and Jacobs, Charles E. and Oliver, Nuria and Curless, Brian and Salesin, David H.},
  title     = {Image Analogies},
  year      = {2001},
  isbn      = {158113374X},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/383259.383295},
  doi       = {10.1145/383259.383295},
  abstract  = {This paper describes a new framework for processing images by example, called “image analogies.” The framework involves two stages: a design phase, in which a pair of images, with one image purported to be a “filtered” version of the other, is presented as “training data”; and an application phase, in which the learned filter is applied to some new target image in order to create an “analogous” filtered result. Image analogies are based on a simple multi-scale autoregression, inspired primarily by recent results in texture synthesis. By choosing different types of source image pairs as input, the framework supports a wide variety of “image filter” effects, including traditional image filters, such as blurring or embossing; improved texture synthesis, in which some textures are synthesized with higher quality than by previous approaches; super-resolution, in which a higher-resolution image is inferred from a low-resolution source; texture transfer, in which images are “texturized” with some arbitrary source texture; artistic filters, in which various drawing and painting styles are synthesized based on scanned real-world examples; and texture-by-numbers, in which realistic scenes, composed of a variety of textures, are created using a simple painting interface.},
  booktitle = {Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques},
  pages     = {327–340},
  numpages  = {14},
  keywords  = {texture-by-numbers, texture transfer, non-photorealistic rendering, autoregression, texture synthesis, Markov random fields, example-based rendering},
  series    = {SIGGRAPH '01}
}

@article{7874180,
  author  = {Elad, Michael and Milanfar, Peyman},
  journal = {IEEE Transactions on Image Processing},
  title   = {Style Transfer Via Texture Synthesis},
  year    = {2017},
  volume  = {26},
  number  = {5},
  pages   = {2338-2351},
  doi     = {10.1109/TIP.2017.2678168}
}

@inproceedings{Gatys_2016_CVPR,
  author    = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  title     = {Image Style Transfer Using Convolutional Neural Networks},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2016}
}

@inproceedings{10.1007/978-3-319-46475-6_43,
  author    = {Johnson, Justin
               and Alahi, Alexandre
               and Fei-Fei, Li},
  editor    = {Leibe, Bastian
               and Matas, Jiri
               and Sebe, Nicu
               and Welling, Max},
  title     = {Perceptual Losses for Real-Time Style Transfer and Super-Resolution},
  booktitle = {Computer Vision -- ECCV 2016},
  year      = {2016},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {694--711},
  abstract  = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
  isbn      = {978-3-319-46475-6}
}

@inproceedings{10.5555/3172077.3172198,
  author    = {Li, Yanghao and Wang, Naiyan and Liu, Jiaying and Hou, Xiaodi},
  title     = {Demystifying Neural Style Transfer},
  year      = {2017},
  isbn      = {9780999241103},
  publisher = {AAAI Press},
  abstract  = {Neural Style Transfer [Gatys et al. , 2016] has recently demonstrated very exciting results which catches eyes in both academia and industry. Despite the amazing results, the principle of neural style transfer, especially why the Gram matrices could represent style remains unclear. In this paper, we propose a novel interpretation of neural style transfer by treating it as a domain adaptation problem. Specifically, we theoretically show that matching the Gram matrices of feature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with the second order polynomial kernel. Thus, we argue that the essence of neural style transfer is to match the feature distributions between the style images and the generated images. To further support our standpoint, we experiment with several other distribution alignment methods, and achieve appealing results. We believe this novel interpretation connects these two important research fields, and could enlighten future researches.},
  booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
  pages     = {2230–2236},
  numpages  = {7},
  location  = {Melbourne, Australia},
  series    = {IJCAI'17}
}

@comment{
  文字の効果や装飾のStyle Transferに関する研究
}

@inproceedings{Yang_2017_CVPR,
  author    = {Yang, Shuai and Liu, Jiaying and Lian, Zhouhui and Guo, Zongming},
  title     = {Awesome Typography: Statistics-Based Text Effects Transfer},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {July},
  year      = {2017}
}

@inproceedings{Yang2019Controllable,
  title     = {Controllable Artistic Text Style Transfer via Shape-Matching GAN},
  author    = {Yang, Shuai and Wang, Zhangyang and Wang, Zhaowen and Xu, Ning
               and Liu, Jiaying and Guo, Zongming},
  booktitle = {International Conference on Computer Vision},
  year      = {2019}
}

@inproceedings{typography2019,
  author    = {Wang, Wenjing and Liu, Jiaying and Yang, Shuai and Guo, Zongming},
  title     = {Typography with Decor: Intelligent Text Style Transfer},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2019}
}

@inproceedings{Yang2019TETGAN,
  title     = {TET-GAN: Text Effects Transfer via Stylization and Destylization},
  author    = {Yang, Shuai and Liu, Jiaying and Wang, Wenjing and Guo, Zongming},
  booktitle = {AAAI Conference on Artificial Intelligence},
  year      = {2019}
}

@comment{
  Style Transferをロゴに応用した研究
}

@conference{icpram20,
  author       = {Aram Ter{-}Sarkisov.},
  title        = {Network of Steel: Neural Font Style Transfer from Heavy Metal to Corporate Logos},
  booktitle    = {Proceedings of the 9th International Conference on Pattern Recognition Applications and Methods - ICPRAM,},
  year         = {2020},
  pages        = {621-629},
  publisher    = {SciTePress},
  organization = {INSTICC},
  doi          = {10.5220/0009343906210629},
  isbn         = {978-989-758-397-1},
  issn         = {2184-4313}
}

@phdthesis{oai:irdb.nii.ac.jp:01211:0005350653,
  author = {アタルサイハン, ガントゥグス},
  title  = {Typographic Design Generation Using Neural Style Transfer},
  school = {九州大学},
  year   = {2022},
  url    = {https://irdb.nii.ac.jp/01211/0005350653}
}